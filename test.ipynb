{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific anchor tag is not found\n",
      "done loading\n",
      "No more pages to load.\n",
      "done loading\n",
      "next page\n",
      "No more pages to load.\n",
      "done loading\n",
      "No more pages to load.\n",
      "done loading\n",
      "next page\n",
      "next page\n",
      "No more pages to load.\n",
      "done loading\n",
      "No more pages to load.\n",
      "done loading\n",
      "next page\n",
      "next page\n",
      "next page\n",
      "next page\n",
      "next page\n",
      "No more pages to load.\n",
      "done loading\n",
      "No more pages to load.\n",
      "done loading\n",
      "No more pages to load.\n",
      "done loading\n",
      "next page\n",
      "next page\n",
      "next page\n",
      "next page\n",
      "next page\n",
      "next page\n",
      "No more pages to load.\n",
      "done loading\n",
      "The specific anchor tag is not found\n",
      "done loading\n"
     ]
    }
   ],
   "source": [
    "class LinkedInScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://www.linkedin.com/login')\n",
    "        time.sleep(2)\n",
    "\n",
    "        email_elem = self.driver.find_element(By.ID, \"username\")\n",
    "        email_elem.send_keys(self.email)\n",
    "\n",
    "        password_elem = self.driver.find_element(By.ID, \"password\")\n",
    "        password_elem.send_keys(self.password)\n",
    "        password_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        input(\"Please complete the captcha and login manually, then press Enter to continue...\")\n",
    "\n",
    "\n",
    "    def fetch_jobs(self, entries, output_file):\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"company_name\", 'company_link', \"job_title_location_and_work_type\"])  # Header\n",
    "\n",
    "            for entry in entries:\n",
    "                url = entry['company_domain']\n",
    "                if url:\n",
    "                    self.driver.get(url)\n",
    "                    time.sleep(3)  # Allow time for all scripts to load and data to display\n",
    "\n",
    "                    # Handle pagination\n",
    "                    try:\n",
    "                        link = self.driver.find_element(By.CSS_SELECTOR, \"a.artdeco-button.artdeco-button--primary.artdeco-button--3.flex-shrink-zero\")\n",
    "                        link.click()\n",
    "                        time.sleep(4)  # Wait for the new tab to open\n",
    "\n",
    "                        original_window = self.driver.current_window_handle\n",
    "                        new_window = [window for window in self.driver.window_handles if window != original_window][0]\n",
    "                        self.driver.switch_to.window(new_window)\n",
    "                        time.sleep(3)  # Allow some time for the new tab to load content\n",
    "                        \n",
    "                        current_page = 1\n",
    "                        anchor_tag_case = 1\n",
    "\n",
    "                        while True:\n",
    "                            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                            ul_tag = soup.find('ul', class_='scaffold-layout__list-container')\n",
    "                            if ul_tag:\n",
    "                                li_tags = ul_tag.find_all('li')\n",
    "                                for li in li_tags:\n",
    "                                    div_tag = li.find('div', class_='flex-grow-1 artdeco-entity-lockup__content ember-view')\n",
    "                                    if div_tag:\n",
    "                                        job_title = div_tag.find('strong')\n",
    "                                        company_name = div_tag.find('span', class_='job-card-container__primary-description')\n",
    "                                        country_and_work_type = div_tag.find('li', class_='job-card-container__metadata-item')\n",
    "                                        company_url = url\n",
    "                                        \n",
    "                                        job_title = (job_title.text).strip().replace('\\n', '') if job_title else \"N/A\"\n",
    "                                        country_and_work_type = (country_and_work_type.text).strip().replace('\\n', '') if country_and_work_type else \"N/A\"\n",
    "                                        job_title_location_and_work_type = job_title + \" - \" + country_and_work_type\n",
    "\n",
    "                                        writer.writerow([\n",
    "                                            (company_name.text).strip().replace('\\n', '') if company_name else \"N/A\",\n",
    "                                            company_url,\n",
    "                                            job_title_location_and_work_type\n",
    "                                        ])  \n",
    "                            else:\n",
    "                                print(\"UL tag not found for company URL: \", url)\n",
    "                                break\n",
    "\n",
    "                            # Check for next page\n",
    "                            current_page += 1\n",
    "                            next_button_selector = f\"button[aria-label='Page {current_page}']\"\n",
    "                            next_buttons = self.driver.find_elements(By.CSS_SELECTOR, next_button_selector)\n",
    "\n",
    "                            if next_buttons:\n",
    "                                # print(next_buttons[0], 'next button is here')\n",
    "                                next_buttons[0].click()\n",
    "                                time.sleep(5)  # Wait for the next page to load\n",
    "                                print('next page')\n",
    "                            else:\n",
    "                                print(\"No more pages to load.\")\n",
    "                                break\n",
    "\n",
    "                    except NoSuchElementException:\n",
    "                        writer.writerow([\n",
    "                                        \n",
    "                                        entry['company_name'],\n",
    "                                        entry['company_domain'],\n",
    "                                        ''\n",
    "                                    ]),\n",
    "                        print(\"The specific anchor tag is not found\")\n",
    "                        anchor_tag_case = 0\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "                        \n",
    "                    finally:\n",
    "                        print('done loading')\n",
    "                        if anchor_tag_case == 1:\n",
    "                            self.driver.close()  # Close the new window\n",
    "                            if original_window in self.driver.window_handles:\n",
    "                                self.driver.switch_to.window(original_window)\n",
    "                            else:\n",
    "                                print(\"Original window has been closed.\")\n",
    "                    anchor_tag_case = 1        \n",
    "                    time.sleep(1)  # Short wait before processing next entry\n",
    "                            \n",
    "            self.driver.quit()\n",
    "\n",
    "def read_csv(file_path):\n",
    "    with open(file_path, newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        entries = [row for row in reader]\n",
    "    return entries\n",
    "\n",
    "# Usage\n",
    "username = \"akber.anwer1234@gmail.com\" # linkedin username\n",
    "password = \"Makber@8472\" # linkedin password\n",
    "input_file = 'test_input_file.csv' # column name -> company_name, company_domain\n",
    "output_file = 'test_output_file.csv' # no need for columns, it will be automatically created\n",
    "\n",
    "scraper = LinkedInScraper(username, password)\n",
    "scraper.login()\n",
    "\n",
    "entries = read_csv(input_file)\n",
    "\n",
    "scraper.fetch_jobs(entries, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_link</th>\n",
       "      <th>job_title_location_and_work_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TestCase</td>\n",
       "      <td>https://www.linkedin.com/company/hawklogix-pak...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abrigo</td>\n",
       "      <td>https://www.linkedin.com/company/weareabrigo/j...</td>\n",
       "      <td>Cloud Security Engineer I - Raleigh, NC (On-site)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abrigo</td>\n",
       "      <td>https://www.linkedin.com/company/weareabrigo/j...</td>\n",
       "      <td>Senior Data Scientist - United States (Remote)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abrigo</td>\n",
       "      <td>https://www.linkedin.com/company/weareabrigo/j...</td>\n",
       "      <td>Principal Data Engineer - United States (Remote)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abrigo</td>\n",
       "      <td>https://www.linkedin.com/company/weareabrigo/j...</td>\n",
       "      <td>Senior Cloud Operations Engineer - United Stat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company_name                                       company_link  \\\n",
       "0     TestCase  https://www.linkedin.com/company/hawklogix-pak...   \n",
       "1       Abrigo  https://www.linkedin.com/company/weareabrigo/j...   \n",
       "2       Abrigo  https://www.linkedin.com/company/weareabrigo/j...   \n",
       "3       Abrigo  https://www.linkedin.com/company/weareabrigo/j...   \n",
       "4       Abrigo  https://www.linkedin.com/company/weareabrigo/j...   \n",
       "\n",
       "                    job_title_location_and_work_type  \n",
       "0                                                NaN  \n",
       "1  Cloud Security Engineer I - Raleigh, NC (On-site)  \n",
       "2     Senior Data Scientist - United States (Remote)  \n",
       "3   Principal Data Engineer - United States (Remote)  \n",
       "4  Senior Cloud Operations Engineer - United Stat...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'test_output_file.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'company_name' and 'company_link', aggregate 'job_title' and 'country_and_work_type'\n",
    "# Convert NaN values to empty strings\n",
    "df['job_title_location_and_work_type'] = df['job_title_location_and_work_type'].fillna('')\n",
    "# df['country_and_work_type'] = df['country_and_work_type'].fillna('')\n",
    "df['company_link'] = df['company_link'].apply(lambda x: x.replace('jobs/', ''))\n",
    "\n",
    "grouped = df.groupby(['company_name', 'company_link']).agg({\n",
    "    'job_title_location_and_work_type': lambda x: '\\n'.join(x),\n",
    "    # 'country_and_work_type': lambda x: '\\n'.join(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Save the result to CSV and Excel files\n",
    "grouped.to_csv('grouped_data.csv', index=False)\n",
    "grouped.to_excel('grouped_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file names to merge\n",
    "file_names = ['book_1_to_128_jd.csv', 'book_128_to_144_jd.csv', 'book_144_to_all_jd.csv']\n",
    "\n",
    "# Read each file into a DataFrame\n",
    "dfs = [pd.read_csv(file) for file in file_names]\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv('merged_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('merged_file.csv')\n",
    "\n",
    "# Function to extract and remove nature of job from the string\n",
    "def extract_nature_of_job(job_string):\n",
    "    job_string = job_string.strip()\n",
    "    nature_of_job = \"\"\n",
    "    if job_string.endswith(\"(On-site)\"):\n",
    "        nature_of_job = \"On-site\"\n",
    "        job_string = job_string[:-len(\"(On-site)\")].strip()\n",
    "    elif job_string.endswith(\"(Remote)\"):\n",
    "        nature_of_job = \"Remote\"\n",
    "        job_string = job_string[:-len(\"(Remote)\")].strip()\n",
    "    elif job_string.endswith(\"(Hybrid)\"):\n",
    "        nature_of_job = \"Hybrid\"\n",
    "        job_string = job_string[:-len(\"(Hybrid)\")].strip()\n",
    "    return job_string, nature_of_job\n",
    "\n",
    "# Process each entry in the specified column\n",
    "new_column_values = []\n",
    "for entry in df['Country_And_Work_Type']:\n",
    "    new_entry, nature_of_job = extract_nature_of_job(entry)\n",
    "    new_column_values.append(nature_of_job)\n",
    "    # Update the original entry in the DataFrame\n",
    "    df['Country_And_Work_Type'] = df['Country_And_Work_Type'].apply(lambda x: x.replace('(On-site)', ''))\n",
    "    df['Country_And_Work_Type'] = df['Country_And_Work_Type'].apply(lambda x: x.replace('(Remote)', ''))\n",
    "    df['Country_And_Work_Type'] = df['Country_And_Work_Type'].apply(lambda x: x.replace('(Hybrid)', ''))\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "df['nature_of_job'] = new_column_values\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "df['linked_link'] = ''\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('updated_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akber\\AppData\\Local\\Temp\\ipykernel_63884\\4018384694.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('updated_file.csv')\n",
    "\n",
    "# Fill NaN values with empty strings\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'Company': 'company_name', \n",
    "                   'Job_Title': 'job_title', \n",
    "                   'Country_And_Work_Type': 'country',\n",
    "                   'nature_of_job': 'job_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and float64 columns for key 'company_name'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_1_to_all.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Merge the DataFrames based on the \"name\" column\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompany_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Update the \"linkedin_link\" column with the \"domain\" values where the names match\u001b[39;00m\n\u001b[0;32m     11\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mloc[merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_link\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDomain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:169\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    155\u001b[0m         left_df,\n\u001b[0;32m    156\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:804\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 804\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py:1479\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m   1475\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1476\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1477\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[0;32m   1478\u001b[0m     ):\n\u001b[1;32m-> 1479\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and float64 columns for key 'company_name'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv('updated_file.csv')\n",
    "df2 = pd.read_csv('book_1_to_all.csv')\n",
    "\n",
    "# Merge the DataFrames based on the \"name\" column\n",
    "merged_df = pd.merge(df1, df2, on='company_name', how='inner')\n",
    "\n",
    "# Update the \"linkedin_link\" column with the \"domain\" values where the names match\n",
    "merged_df.loc[merged_df['company_name'].notnull(), 'company_link'] = merged_df['Domain']\n",
    "\n",
    "# Drop the \"domain\" column as it is no longer needed\n",
    "merged_df.drop('Domain', axis=1, inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('updated_file_with_company_link.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files into DataFrames\n",
    "df = pd.read_csv('updated_file_with_company_link.csv')\n",
    "\n",
    "df['company_link'] = df['company_link'].apply(lambda x: x.replace('jobs/', ''))\n",
    "\n",
    "df.to_csv('updated_file_with_company_link_again.csv', index=False)\n",
    "df.to_excel('updated_file_with_company_link_again.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
