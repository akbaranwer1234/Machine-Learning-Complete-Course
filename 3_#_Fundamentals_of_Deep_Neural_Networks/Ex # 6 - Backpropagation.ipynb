{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Backpropagation Single Variate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defini the network architecture\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 1\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the weights and biases\n",
    "\n",
    "w1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.random.rand(1, hidden_size)\n",
    "w2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.random.rand(1, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward (input_data):\n",
    "    z1 = np.dot(input_data, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    output = sigmoid(z2)\n",
    "    return a1, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate \n",
    "def loss (predicted, target):\n",
    "    return np.mean((predicted - target)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward (input_data, target, learning_rate):\n",
    "    global w1, w2, b1, b2 #declare these as global so they can be modified\n",
    "\n",
    "    #calculate the gradient of the loss with respect to the output\n",
    "    a1, output = forward(input_data) #get a1 and output from the forward pass\n",
    "    output_error = output - target\n",
    "    \n",
    "    #calculates gradients for weights and biases in the output layer\n",
    "    dw2 = np.dot(a1.T, output_error)\n",
    "    db2 = np.sum(output_error, axis=0)\n",
    "    \n",
    "    #calculate gradients for weights and biases in the hidden layer\n",
    "    hidden_error = np.dot(output_error, w2.T) * (a1 * (1 - a1))\n",
    "    dw1 = np.dot(input_data.T, hidden_error)\n",
    "    db1 = np.sum(hidden_error, axis=0)\n",
    "    \n",
    "    #update weights and biases\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.1606045948309189\n",
      "Epoch 100: loss 0.06877819829832899\n",
      "Epoch 200: loss 0.027170691801158376\n",
      "Epoch 300: loss 0.012680057354919581\n",
      "Epoch 400: loss 0.0069467285414194875\n",
      "Epoch 500: loss 0.004275445787671714\n",
      "Epoch 600: loss 0.002858655974182389\n",
      "Epoch 700: loss 0.0020305734546652923\n",
      "Epoch 800: loss 0.0015096525347742954\n",
      "Epoch 900: loss 0.001162770127466281\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    input_data = np.array([[0], [1]])\n",
    "    target = np.array([[1], [0]])\n",
    "    \n",
    "    #forward pass\n",
    "    a1, predicted = forward(input_data)\n",
    "    \n",
    "    #compute loss\n",
    "    current_loss = loss(predicted, target)\n",
    "    \n",
    "    #backward pass\n",
    "    backward(input_data, target, learning_rate)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: loss {current_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 0.5: Prediction: 0.380091\n"
     ]
    }
   ],
   "source": [
    "input_data = 0.5\n",
    "_, predicted = forward(input_data)\n",
    "print(f'Input: {input_size}: Prediction: {predicted[0][0]:3f}')\n",
    "# print(f'Input: {input_size}: Prediction: {predicted[0][0]:3f}, \"test\": {_[0][0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Backpropagation Multivariate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defini the network architecture\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SigmoidFx\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the weights and biases\n",
    "\n",
    "w1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.random.rand(1, hidden_size)\n",
    "w2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.random.rand(1, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward (input_data):\n",
    "    z1 = np.dot(input_data, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    output = sigmoid(z2)\n",
    "    return a1, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate \n",
    "def loss (predicted, target):\n",
    "    return np.mean((predicted - target)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward (input_data, target, learning_rate):\n",
    "    global w1, w2, b1, b2 #declare these as global so they can be modified\n",
    "\n",
    "    #calculate the gradient of the loss with respect to the output\n",
    "    a1, output = forward(input_data) #get a1 and output from the forward pass\n",
    "    output_error = output - target\n",
    "    \n",
    "    #calculates gradients for weights and biases in the output layer\n",
    "    dw2 = np.dot(a1.T, output_error)\n",
    "    db2 = np.sum(output_error, axis=0)\n",
    "    \n",
    "    #calculate gradients for weights and biases in the hidden layer\n",
    "    hidden_error = np.dot(output_error, w2.T) * (a1 * (1 - a1))\n",
    "    dw1 = np.dot(input_data.T, hidden_error)\n",
    "    db1 = np.sum(hidden_error, axis=0)\n",
    "    \n",
    "    #update weights and biases\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.3606515349410203\n",
      "Epoch 100: loss 0.2509884526306542\n",
      "Epoch 200: loss 0.2495598908957077\n",
      "Epoch 300: loss 0.2476955939374783\n",
      "Epoch 400: loss 0.24326783838967908\n",
      "Epoch 500: loss 0.23014906716203945\n",
      "Epoch 600: loss 0.19459171313667345\n",
      "Epoch 700: loss 0.131046030878966\n",
      "Epoch 800: loss 0.06369295403301803\n",
      "Epoch 900: loss 0.026118234547952236\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    target = np.array([[0], [1], [1], [0]])\n",
    "    \n",
    "    #forward pass\n",
    "    a1, predicted = forward(input_data)\n",
    "    \n",
    "    #compute loss\n",
    "    current_loss = loss(predicted, target)\n",
    "    \n",
    "    #backward pass\n",
    "    backward(input_data, target, learning_rate)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: loss {current_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15012591]\n",
      " [0.92126791]\n",
      " [0.87463893]\n",
      " [0.05445411]]\n"
     ]
    }
   ],
   "source": [
    "_, predicted = forward(np.array([[1, 1], [1, 0], [0, 1], [0, 0]]))\n",
    "print(f'{predicted}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
