{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This content was scrapped on **\"Pakistan\"** origing based news websites including \n",
    "\n",
    "**\"DAWN NEWS\"**, **\"THE NATION NEWS\"**, & **\"THE NEWS\"** in which i prefer to target their\n",
    "\n",
    "**\"Opinions\"** section in which i scrapped **\"Editorials\"**, **\"Columns\"**, & **\"Letters to the editors\"** of these news paper from\n",
    "\n",
    "**\"JANUARY 1st, 2008\"** to **\"MARCH 10th, 2024\"** which consists a huge number of content from these articles.\n",
    "\n",
    "Here is a short description of the content:\n",
    "\n",
    "**COLUMNS:** Dawn News (**29531**), The Nation News (**37937**), THE NEWS (**32562**)\n",
    "\n",
    "**EDITORIALS:** Dawn News (**24161**), The Nation News (**35928**), THE NEWS (**21598**)\n",
    "\n",
    "**LETTERS TO THE EDITORS:** Dawn News (**23895**), The Nation News (**12428**), THE NEWS (**34298**)\n",
    "\n",
    "*******************************************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Importing the requests library to make HTTP requests\n",
    "from bs4 import BeautifulSoup  # Importing BeautifulSoup for HTML parsing\n",
    "import csv  # Importing csv module for handling CSV files\n",
    "from selenium import webdriver  # Importing the Selenium webdriver for browser automation\n",
    "from datetime import datetime, timedelta  # Importing datetime and timedelta for working with dates\n",
    "import pandas as pd  # Importing pandas for data manipulation and analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dawn News Scraper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DawnNews:\n",
    "    def __init__(self, url, headers):\n",
    "        self.url = url  # Assigning the URL of the DawnNews website to the object's url attribute\n",
    "        self.headers = headers  # Assigning the headers to be used for making HTTP requests to the object's headers attribute\n",
    "        self.driver = webdriver.Chrome()  # Initializing a Chrome webdriver instance\n",
    "        \n",
    "    def parse_articles(self):\n",
    "        try:\n",
    "            response = requests.get(self.url, headers=self.headers)  # Sending an HTTP GET request to the provided URL with the specified headers\n",
    "            # print(response.content)\n",
    "            response.raise_for_status()  # Checking for any HTTP request errors\n",
    "            soup = BeautifulSoup(response.text, 'lxml')  # Creating a BeautifulSoup object to parse the HTML content of the response\n",
    "            articles = (soup.find_all('article', {'class': 'jeg_post jeg_pl_sm format-standard column-category-border-wrapper'}))  # Finding all articles on the page with the specified class\n",
    "            \n",
    "            for article in articles:  # Iterating over each article found\n",
    "                columnistLink = article.find('div', {'class': 'jeg_thumb'}).find('a')['href']  # Extracting the link of the columnist\n",
    "                columnTitle = article.find('h3', {'class': 'jeg_post_title'}).find('a').text  # Extracting the title of the article\n",
    "                columnLink = article.find('h3', {'class': 'jeg_post_title'}).find('a')['href']  # Extracting the link of the article\n",
    "                columnistName = article.find('div', {'class': 'jeg_meta_author'}).find('a').text  # Extracting the name of the columnist\n",
    "                columPublishDate = article.find('div', {'class': 'jeg_meta_date'}).text  # Extracting the publication date of the article\n",
    "                \n",
    "                # Printing out the extracted information for each article\n",
    "                print(columnistName, columnTitle, columPublishDate, columnLink, columnistLink)\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:  # Catching any HTTP request exceptions\n",
    "            print(f'Error: {e}')  # Printing out the error message if an exception occurs\n",
    "\n",
    "            \n",
    "    # def scrap_columns_section_list(self):\n",
    "    #     try:\n",
    "    #         self.driver.get(self.url)\n",
    "    #         wait = WebDriverWait(self.driver, 3)\n",
    "    #         load_more_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Load More\")]')))\n",
    "\n",
    "    #         while load_more_button:\n",
    "    #             load_more_button.click()\n",
    "    #             # Wait for new articles to load\n",
    "    #             wait.until(EC.presence_of_all_elements_located((By.XPATH, '//article[@class=\"jeg_post jeg_pl_sm format-standard column-category-border-wrapper\"]')))\n",
    "    #             soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "    #             self.parse_articles(soup)\n",
    "                \n",
    "    #             # Check if there are more pages to load\n",
    "    #             load_more_button = self.driver.find_element_by_xpath('//button[contains(text(), \"Load More\")]')\n",
    "\n",
    "    #     finally:\n",
    "    #         self.driver.quit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DawnObject = DawnNews(  # Creating an instance of the DawnNews class with the specified parameters\n",
    "                        'https://www.nation.com.pk/columns',  # Providing the URL of the DawnNews website to scrape\n",
    "                        headers={  # Providing the headers to be used for making HTTP requests\n",
    "                              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',  # User-Agent header to simulate a web browser\n",
    "                              'Accept-Language': 'en-US,en;q=0.9',  # Accept-Language header to specify preferred languages for content\n",
    "                              'Accept-Encoding': 'gzip, deflate, br',  # Accept-Encoding header for specifying acceptable content encoding\n",
    "                              'Connection': 'keep-alive',  # Connection header to keep the connection open\n",
    "                        }\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DawnObject.parse_articles()  # Calling the parse_articles method of the DawnObject instance\n",
    "# DawnObject.scrap_columns_section_list()  # This line is commented out and not executed because the method is not implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Nation News Column Scrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NationScraper:\n",
    "    def __init__(self, url, headers):\n",
    "        self.base_url = url  # Assigning the base URL of the Nation website to the object's base_url attribute\n",
    "        self.headers = headers  # Assigning the headers to be used for making HTTP requests to the object's headers attribute\n",
    "\n",
    "    def generate_date_range(self, start_date, end_date):\n",
    "        # Generating a range of dates between start_date and end_date\n",
    "        start = datetime.strptime(start_date, '%d-%b-%Y')  # Converting the start_date string to a datetime object\n",
    "        end = datetime.strptime(end_date, '%d-%b-%Y')  # Converting the end_date string to a datetime object\n",
    "        delta = timedelta(days=1)  # Defining a timedelta object representing one day\n",
    "        while start <= end:  # Looping through each date in the range\n",
    "            yield start.strftime('%d-%b-%Y')  # Yielding the current date formatted as 'dd-MMM-YYYY'\n",
    "            start += delta  # Moving to the next date in the range\n",
    "\n",
    "    def scrap_columns_by_date(self, date):\n",
    "        # Scraping columns for a specific date\n",
    "        url = f\"{self.base_url}{date}/columns\"  # Constructing the URL for the specific date\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)  # Sending an HTTP GET request to the constructed URL with the specified headers\n",
    "            response.raise_for_status()  # Checking for any HTTP request errors\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')  # Creating a BeautifulSoup object to parse the HTML content of the response\n",
    "            self.parse_articles(soup, date)  # Parsing the articles from the response\n",
    "        except requests.exceptions.RequestException as e:  # Handling any HTTP request exceptions\n",
    "            print(f\"Error scraping data for {date}: {e}\")  # Printing out an error message if an exception occurs\n",
    "\n",
    "    def parse_articles(self, soup, date):\n",
    "        # Parsing articles from the HTML content\n",
    "        articles_list = []  # Creating an empty list to store information about articles\n",
    "        articles = (soup.find_all('article', {'class': 'jeg_post jeg_pl_sm format-standard column-category-border-wrapper'}))  # Finding all articles on the page with the specified class\n",
    "        if articles:  # Checking if articles are found for the given date\n",
    "            print(f\"Articles for {date}:\")  # Printing the date for which articles are being parsed\n",
    "            for article in articles:  # Iterating over each article found\n",
    "                # Initializing variables with default values\n",
    "                columnistLink = 'NF'\n",
    "                columnTitle = 'NF'\n",
    "                columnLink = 'NF'\n",
    "                columnistName = 'NF'\n",
    "                columPublishDate = 'NF'\n",
    "\n",
    "                # Extracting information from various HTML elements within the article\n",
    "                thumb_div = article.find('div', {'class': 'jeg_thumb'})\n",
    "                if thumb_div:\n",
    "                    link_a = thumb_div.find('a')\n",
    "                    if link_a and 'href' in link_a.attrs:\n",
    "                        columnistLink = link_a['href']\n",
    "\n",
    "                title_h3 = article.find('h3', {'class': 'jeg_post_title'})\n",
    "                if title_h3:\n",
    "                    link_a = title_h3.find('a')\n",
    "                    if link_a and 'href' in link_a.attrs:\n",
    "                        columnLink = link_a['href']\n",
    "                    columnTitle = link_a.text\n",
    "\n",
    "                author_div = article.find('div', {'class': 'jeg_meta_author'})\n",
    "                if author_div:\n",
    "                    author_a = author_div.find('a')\n",
    "                    if author_a:\n",
    "                        columnistName = author_a.text\n",
    "\n",
    "                date_div = article.find('div', {'class': 'jeg_meta_date'})\n",
    "                if date_div:\n",
    "                    columPublishDate = date_div.text\n",
    "                \n",
    "                # Adding extracted information to the articles_list\n",
    "                articles_list.append({\n",
    "                    'columnistName': columnistName,\n",
    "                    'columnTitle': columnTitle,\n",
    "                    'columPublishDate': columPublishDate,\n",
    "                    'columnLink': columnLink,\n",
    "                    'columnistLink': columnistLink\n",
    "                })\n",
    "                \n",
    "                # Printing out information about each article\n",
    "                print(columnistName, columnTitle, columPublishDate, columnLink, columnistLink)\n",
    "                \n",
    "                # Saving articles to a CSV file\n",
    "                self.save_to_csv(articles_list)\n",
    "        else:\n",
    "            print(f\"No column found for {date}\")  # Printing a message if no articles are found for the given date\n",
    "    \n",
    "    def save_to_csv(self, articles_list):\n",
    "        # Saving articles to a CSV file\n",
    "        with open('nation_columns.csv', mode='a', newline='', encoding='utf-8') as file:  # Opening the CSV file in append mode\n",
    "            fieldnames = ['columnistName', 'columnTitle', 'columPublishDate', 'columnLink', 'columnistLink']  # Defining field names for CSV columns\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)  # Creating a CSV DictWriter object\n",
    "            if file.tell() == 0:  # Checking if the file pointer is at the beginning of the file (empty file)\n",
    "                writer.writeheader()  # Writing header row if the file is empty\n",
    "            for article in articles_list:  # Iterating over each article in the articles_list\n",
    "                writer.writerow(article)  # Writing article information to the CSV file row by row\n",
    "\n",
    "    def scrape_date_range(self, start_date, end_date):\n",
    "        # Scraping columns for a range of dates\n",
    "        for date in self.generate_date_range(start_date, end_date):  # Iterating over each date in the generated date range\n",
    "            self.scrap_columns_by_date(date)  # Scraping columns for the current date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NationObject = NationScraper(  # Creating an instance of the NationScraper class with the specified parameters\n",
    "    'https://www.nation.com.pk/archives/',  # Providing the base URL of the Nation website to scrape\n",
    "    headers={  # Providing the headers to be used for making HTTP requests\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',  # User-Agent header to simulate a web browser\n",
    "        'Accept-Language': 'en-US,en;q=0.9',  # Accept-Language header to specify preferred languages for content\n",
    "        'Accept-Encoding': 'gzip, deflate, br',  # Accept-Encoding header for specifying acceptable content encoding\n",
    "        'Connection': 'keep-alive',  # Connection header to keep the connection open\n",
    "    }\n",
    ")\n",
    "\n",
    "# start_date = '01-Jan-2008'\n",
    "# end_date = '10-Mar-2024'\n",
    "start_date = '08-Mar-2024'  # Setting the start date for the scraping range\n",
    "end_date = '10-Mar-2024'  # Setting the end date for the scraping range\n",
    "\n",
    "NationObject.scrape_date_range(start_date, end_date)  # Calling the scrape_date_range method of the NationObject instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of the NationScraper class with the specified parameters\n",
    "NationObject = NationScraper(\n",
    "    'https://www.nation.com.pk/archives/',  # Providing the base URL of the Nation website to scrape\n",
    "    headers={  # Providing the headers to be used for making HTTP requests\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',  # User-Agent header to simulate a web browser\n",
    "        'Accept-Language': 'en-US,en;q=0.9',  # Accept-Language header to specify preferred languages for content\n",
    "        'Accept-Encoding': 'gzip, deflate, br',  # Accept-Encoding header for specifying acceptable content encoding\n",
    "        'Connection': 'keep-alive',  # Connection header to keep the connection open\n",
    "    }\n",
    ")\n",
    "\n",
    "# Defining the start and end dates for the scraping range\n",
    "start_date = '10-Jan-2012'  # Setting the start date for the scraping range\n",
    "end_date = '10-Mar-2024'  # Setting the end date for the scraping range\n",
    "\n",
    "# Initiating the scraping process for articles published within the specified date range on the Nation website\n",
    "NationObject.scrape_date_range(start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing for articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from a CSV file named 'articles.csv' into a pandas DataFrame\n",
    "df = pd.read_csv('articles.csv')\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows in the DataFrame and summing up the count of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows from the DataFrame and assigning the result back to df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Checking for duplicate rows in the DataFrame after removing duplicates and summing up the count of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of the DataFrame to inspect the changes after removing duplicates\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Nation News Column Content Scrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleScraper:\n",
    "    def __init__(self, csv_file):\n",
    "        # Initializing the ArticleScraper object with the CSV file to read and write data\n",
    "        self.csv_file = csv_file\n",
    "        # Setting headers to be used for making HTTP requests\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "\n",
    "    def scrape_articles(self):\n",
    "        # Opening the CSV file for reading data\n",
    "        with open(self.csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            # Creating a DictReader object to read rows from the CSV file as dictionaries\n",
    "            reader = csv.DictReader(file)\n",
    "            # Getting fieldnames from the reader and adding 'articleContent' as a new field\n",
    "            fieldnames = reader.fieldnames + ['articleContent']\n",
    "            rows = []\n",
    "            # Iterating over each row in the CSV file\n",
    "            for row in reader:\n",
    "                # Extracting the article URL from the row\n",
    "                article_url = row['columnLink']\n",
    "                # Scraping article content for the article URL\n",
    "                article_content = self.scrape_article_content(article_url)\n",
    "                # Adding the scraped article content to the row\n",
    "                row['articleContent'] = article_content\n",
    "                # Adding the modified row to the list of rows\n",
    "                rows.append(row)\n",
    "                # Printing the modified row\n",
    "                print(row)\n",
    "\n",
    "        # Opening the CSV file for writing data\n",
    "        with open(self.csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            # Creating a DictWriter object to write rows to the CSV file\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            # Writing the header row to the CSV file\n",
    "            writer.writeheader()\n",
    "            # Writing the modified rows to the CSV file\n",
    "            writer.writerows(rows)\n",
    "\n",
    "    def scrape_article_content(self, article_url):\n",
    "        try:\n",
    "            # Sending an HTTP GET request to the article URL with specified headers\n",
    "            response = requests.get(article_url, headers=self.headers)\n",
    "            # Checking for any HTTP request errors\n",
    "            response.raise_for_status()\n",
    "            # Creating a BeautifulSoup object to parse the HTML content of the response\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Scraping logic for article content, modify as needed\n",
    "            article_content = 'NP'  # Default value for article content\n",
    "            # Finding the HTML element containing the article content\n",
    "            content = soup.find('div', {'class': 'content-inner news-detail-content-class'})\n",
    "            # Extracting the text content if found\n",
    "            if content:\n",
    "                article_content = content.text\n",
    "\n",
    "            return article_content  # Returning the scraped article content\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handling any HTTP request exceptions\n",
    "            print(f\"Error scraping content from {article_url}: {e}\")\n",
    "            return ''  # Returning an empty string if scraping fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Define the name of the CSV file containing article information\n",
    "csv_file = 'nation_columns.csv'\n",
    "\n",
    "# Create an instance of the ArticleScraper class, providing the CSV file name as an argument\n",
    "scraper = ArticleScraper(csv_file)\n",
    "\n",
    "# Call the scrape_articles() method of the ArticleScraper instance to initiate the scraping process\n",
    "scraper.scrape_articles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Nation News Editorials Scrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NationScraper:\n",
    "    def __init__(self, url, headers):\n",
    "        # Initialize NationScraper object with base URL and headers\n",
    "        self.base_url = url\n",
    "        self.headers = headers\n",
    "\n",
    "    def generate_date_range(self, start_date, end_date):\n",
    "        # Generate a range of dates between start_date and end_date\n",
    "        start = datetime.strptime(start_date, '%d-%b-%Y')\n",
    "        end = datetime.strptime(end_date, '%d-%b-%Y')\n",
    "        delta = timedelta(days=1)\n",
    "        while start <= end:\n",
    "            yield start.strftime('%d-%b-%Y')\n",
    "            start += delta\n",
    "\n",
    "    def scrap_editorials_by_date(self, date):\n",
    "        # Scrape editorials for a specific date\n",
    "        url = f\"{self.base_url}{date}/editorials\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            self.parse_editorials(soup, date)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error scraping data for {date}: {e}\")\n",
    "\n",
    "    def parse_editorials(self, soup, date):\n",
    "        # Parse editorials from the HTML content\n",
    "        editorials_list = []\n",
    "        articles = (soup.find_all('article', {'class': 'jeg_post jeg_pl_md_1 format-standard'}))\n",
    "        if articles:\n",
    "            print(f\"Editorials for {date}:\")\n",
    "            for article in articles:\n",
    "                editorialLink = 'NF'  # Default value for editorial link\n",
    "                editorialName = 'NF'  # Default value for editorial name\n",
    "                editorialPublishDate = 'NF'  # Default value for editorial publish date\n",
    "\n",
    "                # Extracting editorial link\n",
    "                thumb_div = article.find('div', {'class': 'jeg_thumb'})\n",
    "                if thumb_div:\n",
    "                    link_a = thumb_div.find('a')\n",
    "                    if link_a and 'href' in link_a.attrs:\n",
    "                        editorialLink = link_a['href']\n",
    "\n",
    "                # Extracting editorial name\n",
    "                title_h3 = article.find('div', {'class': 'jeg_postblock_content'})\n",
    "                if title_h3:\n",
    "                    link_a = title_h3.find('a')\n",
    "                    if link_a and 'href' in link_a.attrs:\n",
    "                        editorialName = link_a.text\n",
    "\n",
    "                # Extracting editorial publish date\n",
    "                date_div = article.find('div', {'class': 'jeg_meta_date'})\n",
    "                if date_div:\n",
    "                    editorialPublishDate = date_div.text\n",
    "                \n",
    "                # Adding extracted information to the editorials_list\n",
    "                editorials_list.append({\n",
    "                    'editorialName': editorialName,\n",
    "                    'editorialPublishDate': editorialPublishDate,\n",
    "                    'editorialLink': editorialLink,\n",
    "                })\n",
    "                \n",
    "                # Printing out information about each editorial\n",
    "                print(editorialName, editorialPublishDate, editorialLink)\n",
    "                \n",
    "                # Saving editorials to a CSV file\n",
    "                self.save_to_csv(editorials_list)\n",
    "        else:\n",
    "            print(f\"No editorial found for {date}\")\n",
    "    \n",
    "    def save_to_csv(self, articles_list):\n",
    "        # Saving editorials to a CSV file\n",
    "        with open('nation_editorials.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['editorialName', 'editorialPublishDate', 'editorialLink']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for article in articles_list:\n",
    "                writer.writerow(article)\n",
    "\n",
    "    def scrape_date_range(self, start_date, end_date):\n",
    "        # Scrape editorials for a range of dates\n",
    "        for date in self.generate_date_range(start_date, end_date):\n",
    "            self.scrap_editorials_by_date(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of the NationScraper class with the specified parameters\n",
    "NationObject = NationScraper(\n",
    "    'https://www.nation.com.pk/archives/',  # Providing the base URL of the Nation website to scrape\n",
    "    headers={  # Providing the headers to be used for making HTTP requests\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',  # User-Agent header to simulate a web browser\n",
    "        'Accept-Language': 'en-US,en;q=0.9',  # Accept-Language header to specify preferred languages for content\n",
    "        'Accept-Encoding': 'gzip, deflate, br',  # Accept-Encoding header for specifying acceptable content encoding\n",
    "        'Connection': 'keep-alive',  # Connection header to keep the connection open\n",
    "    }\n",
    ")\n",
    "\n",
    "start_date = '01-Jan-2008'  # Setting the start date for the scraping range\n",
    "end_date = '10-Mar-2024'  # Setting the end date for the scraping range\n",
    "\n",
    "# Initiating the scraping process for editorials published within the specified date range on the Nation website\n",
    "NationObject.scrape_date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing for the Nation News article**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from a CSV file named 'nation_editorials.csv' into a pandas DataFrame\n",
    "df = pd.read_csv('nation_editorials.csv')\n",
    "\n",
    "# Displaying the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows in the DataFrame and summing up the count of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows from the DataFrame and assigning the result back to df\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Checking for duplicate rows in the DataFrame after removing duplicates and summing up the count of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of the DataFrame to inspect the changes after removing duplicates\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out rows where the 'editorialName' column contains the word 'Cartoon'\n",
    "df = df[~df['editorialName'].str.contains('Cartoon')]\n",
    "\n",
    "# Displaying the first few rows of the filtered DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Nation News Editorials Content Scrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleScraper:\n",
    "    def __init__(self, csv_file):\n",
    "        # Initialize the ArticleScraper object with the CSV file name and headers for HTTP requests\n",
    "        self.csv_file = csv_file\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "\n",
    "    def scrape_articles(self):\n",
    "        # Open the CSV file for reading\n",
    "        with open(self.csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            # Create a DictReader object to read rows from the CSV file as dictionaries\n",
    "            reader = csv.DictReader(file)\n",
    "            # Define fieldnames for the output CSV file including a new field 'editorialContent'\n",
    "            fieldnames = reader.fieldnames + ['editorialContent']\n",
    "            rows = []\n",
    "            # Iterate over each row in the CSV file\n",
    "            for row in reader:\n",
    "                # Extract the article URL from the row\n",
    "                article_url = row['editorialLink']\n",
    "                # Scrape the article content for the article URL\n",
    "                article_content = self.scrape_article_content(article_url)\n",
    "                # Add the scraped article content to the row\n",
    "                row['editorialContent'] = article_content\n",
    "                # Append the modified row to the list of rows\n",
    "                rows.append(row)\n",
    "                # Print the modified row\n",
    "                print(row)\n",
    "\n",
    "        # Open the CSV file for writing\n",
    "        with open(self.csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            # Create a DictWriter object to write rows to the CSV file\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            # Write the header row to the CSV file\n",
    "            writer.writeheader()\n",
    "            # Write the modified rows to the CSV file\n",
    "            writer.writerows(rows)\n",
    "\n",
    "    def scrape_article_content(self, article_url):\n",
    "        try:\n",
    "            # Send an HTTP GET request to the article URL with specified headers\n",
    "            response = requests.get(article_url, headers=self.headers)\n",
    "            # Check for any HTTP request errors\n",
    "            response.raise_for_status()\n",
    "            # Create a BeautifulSoup object to parse the HTML content of the response\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Scraping logic for article content, modify as needed\n",
    "            article_content = 'NP'  # Default value for article content\n",
    "            # Find the HTML element containing the article content\n",
    "            content = soup.find('div', {'class': 'content-inner news-detail-content-class'})\n",
    "            # Extract the text content if found\n",
    "            if content:\n",
    "                article_content = content.text\n",
    "\n",
    "            return article_content  # Return the scraped article content\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle any HTTP request exceptions\n",
    "            print(f\"Error scraping content from {article_url}: {e}\")\n",
    "            return ''  # Return an empty string if scraping fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Define the name of the CSV file containing editorial information\n",
    "csv_file = 'nation_editorials.csv'\n",
    "\n",
    "# Create an instance of the ArticleScraper class, providing the CSV file name as an argument\n",
    "scraper = ArticleScraper(csv_file)\n",
    "\n",
    "# Call the scrape_articles() method of the ArticleScraper instance to initiate the scraping process\n",
    "scraper.scrape_articles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily Times News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dailytimes.com.pk/opeds/\n",
    "# https://dailytimes.com.pk/opeds/page/2160/\n",
    "\n",
    "# https://dailytimes.com.pk/editorial/\n",
    "# https://dailytimes.com.pk/editorial/page/1023/\n",
    "\n",
    "# https://dailytimes.com.pk/perspectives/\n",
    "# https://dailytimes.com.pk/perspectives/page/357/\n",
    "\n",
    "# https://dailytimes.com.pk/letters/\n",
    "# https://dailytimes.com.pk/letters/page/1346/\n",
    "\n",
    "# https://dailytimes.com.pk/blog/pakistan-blog/\n",
    "# https://dailytimes.com.pk/blog/pakistan-blog/page/259/\n",
    "\n",
    "# https://dailytimes.com.pk/blog/world-blog/\n",
    "# https://dailytimes.com.pk/blog/world-blog/page/44/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DawnNews:\n",
    "    def __init__(self, url, headers):\n",
    "        self.url = url  # Assigning the URL of the DawnNews website to the object's url attribute\n",
    "        self.headers = headers  # Assigning the headers to be used for making HTTP requests to the object's headers attribute\n",
    "        # self.driver = webdriver.Chrome()  # Initializing a Chrome webdriver instance\n",
    "        \n",
    "    def parse_articles(self):\n",
    "        try:\n",
    "            response = requests.get(self.url, headers=self.headers)  # Sending an HTTP GET request to the provided URL with the specified headers\n",
    "            # print(response.content)\n",
    "            response.raise_for_status()  # Checking for any HTTP request errors\n",
    "            soup = BeautifulSoup(response.text, 'lxml')  # Creating a BeautifulSoup object to parse the HTML content of the response\n",
    "            articles = (soup.find('ul', {'class': 'list-group'}))  # Finding all articles on the page with the specified class\n",
    "            all_li = articles.find_all('li') # Finding all articles\n",
    "            # print(all_li) # Printing)\n",
    "            editorials_list = []\n",
    "            \n",
    "            for article in all_li:  # Iterating over each article found\n",
    "                universityLink = article.find('div', {'class': 'col-md-12'}).find('a')['href']  # Extracting the link of the columnist\n",
    "                universityTitle = article.find('div', {'class': 'col-md-12'}).find('a').text  # Extracting the title of the article\n",
    "                universityCity = article.find('i', {'class': 'fa fa-map-marker'}).nextSibling.strip()  # Extracting the link of the article\n",
    "                \n",
    "                # Printing out the extracted information for each article\n",
    "                print(universityTitle, universityCity, universityLink)\n",
    "                \n",
    "                editorials_list.append({\n",
    "                    'universityTitle': universityTitle,\n",
    "                    # 'universityLink': universityLink,\n",
    "                    'universityCity': universityCity,\n",
    "                })\n",
    "                \n",
    "                \n",
    "            # Convert the list of dictionaries to a DataFrame\n",
    "            df = pd.DataFrame(editorials_list)\n",
    "            \n",
    "            # Save DataFrame to CSV file\n",
    "            df.to_csv('Ireland_Universities.csv', index=False)\n",
    "                \n",
    "        \n",
    "        except requests.exceptions.RequestException as e:  # Catching any HTTP request exceptions\n",
    "            print(f'Error: {e}')  # Printing out the error message if an exception occurs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "DawnObject = DawnNews(  # Creating an instance of the DawnNews class with the specified parameters\n",
    "                        'http://study.eu/search?countries=ie&page=20&sort=recommended',  # Providing the URL of the DawnNews website to scrape\n",
    "                        headers={  # Providing the headers to be used for making HTTP requests\n",
    "                              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',  # User-Agent header to simulate a web browser\n",
    "                              'Accept-Language': 'en-US,en;q=0.9',  # Accept-Language header to specify preferred languages for content\n",
    "                              'Accept-Encoding': 'gzip, deflate, br',  # Accept-Encoding header for specifying acceptable content encoding\n",
    "                              'Connection': 'keep-alive',  # Connection header to keep the connection open\n",
    "                        }\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University College Cork (UCC) Cork, Ireland /university/university-college-cork-ucc\n",
      "University of Galway Galway, Ireland /university/university-of-galway\n",
      "University College Cork (UCC) Cork, Ireland /university/university-college-cork-ucc\n",
      "Dublin City University (DCU) Dublin, Ireland /university/dublin-city-university-dcu\n",
      "TU Shannon Athlone, Ireland /university/tu-shannon\n",
      "Athlone Institute of Technology (AIT) Athlone, Ireland /university/athlone-institute-of-technology-ait\n",
      "Dublin City University (DCU) Dublin, Ireland /university/dublin-city-university-dcu\n",
      "University College Cork (UCC) Cork, Ireland /university/university-college-cork-ucc\n",
      "TU Shannon Athlone, Ireland /university/tu-shannon\n",
      "Athlone Institute of Technology (AIT) Athlone, Ireland /university/athlone-institute-of-technology-ait\n"
     ]
    }
   ],
   "source": [
    "DawnObject.parse_articles()  # Calling the parse_articles method of the DawnObject instance\n",
    "# DawnObject.scrap_columns_section_list()  # This line is commented out and not executed because the method is not implemented yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the website\n",
    "# driver.get('https://www.dawn.com/newspaper/column')\n",
    "\n",
    "# # Scroll and extract data\n",
    "# scroll_pause_time = 2  # Adjust as needed\n",
    "# last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# while True:\n",
    "#     # Scroll down to bottom\n",
    "#     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "#     # Wait to load page\n",
    "#     time.sleep(scroll_pause_time)\n",
    "\n",
    "#     # Calculate new scroll height and compare with last scroll height\n",
    "#     new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     if new_height == last_height:\n",
    "#         break\n",
    "#     last_height = new_height\n",
    "\n",
    "# # Extract data from the page\n",
    "# columns = driver.find_elements_by_xpath('//h2[@class=\"story__title\"]/a')\n",
    "\n",
    "# # Print titles of columns\n",
    "# for column in columns:\n",
    "#     print(column.text)\n",
    "\n",
    "# # Close the WebDriver\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git rebase --abort   --continue --skip\n",
    "# git rebase -i HEAD~10\n",
    "# esc :wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
