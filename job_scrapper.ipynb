{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_job_details(company):\n",
    "    url = f\"https://www.linkedin.com/company/{company}/jobs/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return \"Failed to retrieve data\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(soup)\n",
    "    # # print(soup.prettify())\n",
    "    # jobs = []\n",
    "\n",
    "    # # Assuming each job is in an element with the class 'job-listing'\n",
    "    # for job_listing in soup.find_all(class_=\"job-listing\"):\n",
    "    #     job_title = job_listing.find(class_=\"job-title\").text.strip()\n",
    "    #     location = job_listing.find(class_=\"job-location\").text.strip()\n",
    "    #     jobs.append({\n",
    "    #         \"title\": job_title,\n",
    "    #         \"location\": location\n",
    "    #     })\n",
    "\n",
    "    # return jobs\n",
    "\n",
    "# Example usage\n",
    "company_name = \"achievecareers\"\n",
    "job_details = fetch_job_details(company_name)\n",
    "# print(job_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobScrapper:\n",
    "    def __init__(self, url, headers):\n",
    "        self.url = url\n",
    "        self.headers = headers\n",
    "        \n",
    "    def parse_jobs(self):\n",
    "        try:\n",
    "            response = requests.get(self.url, headers=self.headers)  \n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser') \n",
    "            print(soup)\n",
    "            jobs = (soup.find_all('li', {'class': 'artdeco-carousel__item active ember-view'}))  \n",
    "            \n",
    "            for job in jobs:\n",
    "                print(job)\n",
    "                print('------------------------------------------------------------------------------------------------------------------')\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Error: {e}')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobsScrapperObj = JobScrapper(\n",
    "                        'https://www.linkedin.com/company/weareabrigo/jobs/',\n",
    "                        headers = {\n",
    "                            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",\n",
    "                            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "                            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "                            # \"Referer\": \"https://www.google.com/\",\n",
    "                            \"Connection\": \"keep-alive\",\n",
    "                            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "                            \"Cache-Control\": \"no-cache\"\n",
    "                        }\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobsScrapperObj.parse_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class JobScrapper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        # Set up Selenium WebDriver with Chrome\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "    def parse_jobs(self):\n",
    "        try:\n",
    "            self.driver.get(self.url)  # Use Selenium to get the page content\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            print(soup.prettify())  # Print formatted HTML\n",
    "            # Your job parsing logic here...\n",
    "            \n",
    "            # Just an example of finding elements\n",
    "            jobs = soup.find_all('div', {'class': 'artdeco-carousel__content'})\n",
    "            for job in jobs:\n",
    "                print(job)\n",
    "                print('------------------------------------------------------------------------------------------------------------------')\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "        finally:\n",
    "            self.driver.quit()  # Close the browser once done\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class JobScrapper:\n",
    "    def __init__(self, url):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        self.url = url\n",
    "\n",
    "    def parse_jobs(self):\n",
    "        try:\n",
    "            self.driver.get(self.url)\n",
    "            time.sleep(random.uniform(1.5, 3.0))  # Random delay\n",
    "\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            print(soup.prettify())\n",
    "\n",
    "            jobs = soup.find_all('li', {'class': 'artdeco-carousel__item active ember-view'})\n",
    "            for job in jobs:\n",
    "                print(job)\n",
    "                print('------------------------------------------------------------------------------------------------------------------')\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "job_scraper = JobScrapper('https://www.linkedin.com/company/weareabrigo/jobs/')\n",
    "job_scraper.parse_jobs()\n",
    "\n",
    "# Example usage\n",
    "job_scraper = JobScrapper('https://www.linkedin.com/company/weareabrigo/jobs/')\n",
    "job_scraper.parse_jobs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobScrapper:\n",
    "    def __init__(self, url):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        self.url = url\n",
    "\n",
    "    def parse_jobs(self):\n",
    "        try:\n",
    "            self.driver.get(self.url)\n",
    "            time.sleep(random.uniform(1.5, 3.0))  # Random delay\n",
    "\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            # print(soup.prettify())\n",
    "\n",
    "            jobs = soup.find_all('li', {'class': 'artdeco-carousel__item active ember-view'})\n",
    "            print(jobs)\n",
    "            for job in jobs:\n",
    "                print(job)\n",
    "                print('------------------------------------------------------------------------------------------------------------------')\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "job_scraper = JobScrapper('https://www.linkedin.com/company/weareabrigo/jobs/')\n",
    "job_scraper.parse_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "class LinkedInScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")  # Start maximized\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://www.linkedin.com/login')\n",
    "        time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "        # Input email\n",
    "        email_elem = self.driver.find_element(\"id\", \"username\")\n",
    "        email_elem.send_keys(self.email)\n",
    "\n",
    "        # Input password\n",
    "        password_elem = self.driver.find_element(\"id\", \"password\")\n",
    "        password_elem.send_keys(self.password)\n",
    "        password_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)  # Wait for login to complete\n",
    "\n",
    "    def fetch_jobs(self, company_url):\n",
    "        self.driver.get(company_url)\n",
    "        time.sleep(5)  # Wait for the page to load and jobs to be visible\n",
    "\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        # print(soup.prettify())  # Print page content for debugging\n",
    "\n",
    "        jobs = soup.find_all('div', {'class': 'artdeco-card org-jobs-recently-posted-jobs-module container mb3'})\n",
    "        print(jobs)\n",
    "        # for job in jobs:\n",
    "        #     print(job.text)\n",
    "        #     print('------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "        self.driver.quit()\n",
    "\n",
    "# Usage\n",
    "email = \"akber.anwer1234@gmail.com\"\n",
    "password = \"Makber@8472\"\n",
    "scraper = LinkedInScraper(email, password)\n",
    "scraper.login()\n",
    "scraper.fetch_jobs('https://www.linkedin.com/company/weareabrigo/jobs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinkedInScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://www.linkedin.com/login')\n",
    "        time.sleep(2)\n",
    "\n",
    "        email_elem = self.driver.find_element(By.ID, \"username\")\n",
    "        email_elem.send_keys(self.email)\n",
    "\n",
    "        password_elem = self.driver.find_element(By.ID, \"password\")\n",
    "        password_elem.send_keys(self.password)\n",
    "        password_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "\n",
    "    def fetch_jobs(self, company_url):\n",
    "        self.driver.get(company_url)\n",
    "        time.sleep(5)  # Wait for all elements to load\n",
    "\n",
    "        # Click the specific link and handle new window\n",
    "        try:\n",
    "            link = self.driver.find_element(By.CLASS_NAME, \"artdeco-carousel__content\")\n",
    "            link.click()\n",
    "            time.sleep(3)  # Wait for the new window/tab to open\n",
    "\n",
    "            # Switch to the new window\n",
    "            windows = self.driver.window_handles\n",
    "            self.driver.switch_to.window(windows[1])  # Assuming the new window is the second one\n",
    "\n",
    "            # Perform operations in the new window\n",
    "            try:\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                print(soup.prettify())  # For debugging, prints the HTML of the new page\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page source: {e}\")\n",
    "\n",
    "            self.driver.close()  # Close the current tab\n",
    "            self.driver.switch_to.window(windows[0])  # Switch back to the original window\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print(\"The specific link was not found, skipping...\")\n",
    "\n",
    "        self.driver.quit()\n",
    "\n",
    "# Usage\n",
    "email = \"akber.anwer1234@gmail.com\"\n",
    "password = \"Makber@8472\"\n",
    "scraper = LinkedInScraper(email, password)\n",
    "scraper.login()\n",
    "scraper.fetch_jobs('https://www.linkedin.com/company/weareabrigo/jobs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedInScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://www.linkedin.com/login')\n",
    "        time.sleep(2)\n",
    "\n",
    "        email_elem = self.driver.find_element(By.ID, \"username\")\n",
    "        email_elem.send_keys(self.email)\n",
    "\n",
    "        password_elem = self.driver.find_element(By.ID, \"password\")\n",
    "        password_elem.send_keys(self.password)\n",
    "        password_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "\n",
    "    def fetch_jobs(self, company_url):\n",
    "        self.driver.get(company_url)\n",
    "        time.sleep(5)  # Allow time for all scripts to load and data to display\n",
    "\n",
    "        # Click the specific link\n",
    "        try:\n",
    "            link = self.driver.find_element(By.CLASS_NAME, \"artdeco-carousel__content\")\n",
    "            link.click()\n",
    "            time.sleep(5)  # Wait for the content to load after the click\n",
    "            \n",
    "            data = []  # List to store job data\n",
    "\n",
    "            # Scrape content after click, assuming content is loaded on the same page\n",
    "            try:\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                # print(soup.prettify())  # For debugging, prints the HTML of the page\n",
    "                \n",
    "                # Finding the specific ul, li, and div tags\n",
    "                ul_tag = soup.find('ul', class_='scaffold-layout__list-container')\n",
    "                if ul_tag:\n",
    "                    li_tags = ul_tag.find_all('li')\n",
    "                    for li in li_tags:\n",
    "                        div_tag = li.find('div', class_='flex-grow-1 artdeco-entity-lockup__content ember-view')\n",
    "                        if div_tag:\n",
    "                            job_title = div_tag.find('strong')\n",
    "                            company = div_tag.find('span', class_='job-card-container__primary-description')\n",
    "                            country_and_work_type = div_tag.find('li', class_='job-card-container__metadata-item')\n",
    "                            # type = div_tag.find('strong')\n",
    "                            if job_title:\n",
    "                                print(job_title.text)\n",
    "                            else:\n",
    "                                print(\"job_title not found\")\n",
    "                            if company:\n",
    "                                print(company.text)\n",
    "                            else:\n",
    "                                print(\"company not found\")\n",
    "                            if country_and_work_type:\n",
    "                                print(country_and_work_type.text)\n",
    "                            else:\n",
    "                                print(\"country_and_work_type not found\")\n",
    "                            # Collecting data\n",
    "                            row = [\n",
    "                                job_title.text if job_title else \"N/A\",\n",
    "                                company.text if company else \"N/A\",\n",
    "                                country_and_work_type.text if country_and_work_type else \"N/A\"\n",
    "                            ]\n",
    "                            data.append(row)\n",
    "                else:\n",
    "                    print(\"UL tag not found\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page source: {e}\")\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print(\"The specific link was not found, skipping...\")\n",
    "\n",
    "        self.driver.quit()\n",
    "\n",
    "# Usage\n",
    "email = \"akber.anwer1234@gmail.com\"\n",
    "password = \"Makber@8472\"\n",
    "scraper = LinkedInScraper(email, password)\n",
    "scraper.login()\n",
    "scraper.fetch_jobs('https://www.linkedin.com/company/weareabrigo/jobs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"N/A\"\n",
    "    return text.replace('\"', '').replace(',', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedInScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://www.linkedin.com/login')\n",
    "        time.sleep(2)\n",
    "\n",
    "        email_elem = self.driver.find_element(By.ID, \"username\")\n",
    "        email_elem.send_keys(self.email)\n",
    "\n",
    "        password_elem = self.driver.find_element(By.ID, \"password\")\n",
    "        password_elem.send_keys(self.password)\n",
    "        password_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "\n",
    "    def fetch_jobs(self, company_url):\n",
    "        self.driver.get(company_url)\n",
    "        time.sleep(5)  # Allow time for all scripts to load and data to display\n",
    "\n",
    "        with open('jobs_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Job Title\", \"Company\", \"Country/Work Type\"])  # Header row\n",
    "\n",
    "            # Click the specific link\n",
    "            try:\n",
    "                link = self.driver.find_element(By.CLASS_NAME, \"artdeco-carousel__content\")\n",
    "                link.click()\n",
    "                time.sleep(3)  # Wait for the content to load after the click\n",
    "\n",
    "                try:\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    \n",
    "                    ul_tag = soup.find('ul', class_='scaffold-layout__list-container')\n",
    "                    if ul_tag:\n",
    "                        li_tags = ul_tag.find_all('li')\n",
    "                        for li in li_tags:\n",
    "                            div_tag = li.find('div', class_='flex-grow-1 artdeco-entity-lockup__content ember-view')\n",
    "                            if div_tag:\n",
    "                                job_title = div_tag.find('strong')\n",
    "                                company = div_tag.find('span', class_='job-card-container__primary-description')\n",
    "                                country_and_work_type = div_tag.find('li', class_='job-card-container__metadata-item')\n",
    "\n",
    "                                # Write data directly to CSV\n",
    "                                writer.writerow([\n",
    "                                    job_title.text if job_title else \"N/A\",\n",
    "                                    company.text if company else \"N/A\",\n",
    "                                    country_and_work_type.text if country_and_work_type else \"N/A\"\n",
    "                                ])\n",
    "                    else:\n",
    "                        print(\"UL tag not found\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing page source: {e}\")\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print(\"The specific link was not found, skipping...\")\n",
    "\n",
    "        self.driver.quit()\n",
    "\n",
    "# Usage\n",
    "email = \"akber.anwer1234@gmail.com\"\n",
    "password = \"Makber@8472\"\n",
    "scraper = LinkedInScraper(email, password)\n",
    "scraper.login()\n",
    "scraper.fetch_jobs('https://www.linkedin.com/company/weareabrigo/jobs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "class LinkedInScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://www.linkedin.com/login')\n",
    "        time.sleep(2)\n",
    "\n",
    "        email_elem = self.driver.find_element(By.ID, \"username\")\n",
    "        email_elem.send_keys(self.email)\n",
    "\n",
    "        password_elem = self.driver.find_element(By.ID, \"password\")\n",
    "        password_elem.send_keys(self.password)\n",
    "        password_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        input(\"Please complete the captcha and login manually, then press Enter to continue...\")\n",
    "\n",
    "\n",
    "    def fetch_jobs(self, entries):\n",
    "        with open('Book1_110.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Company\", \"Job_Title\", \"Country_And_Work_Type\"])  # Header \n",
    "            print(entries)\n",
    "\n",
    "            for entry in entries:\n",
    "                url = entry['Domain']  # Modify as necessary based on actual data format\n",
    "                if url:\n",
    "                    self.driver.get(url)\n",
    "                    time.sleep(5)  # Allow time for all scripts to load and data to display\n",
    "                    \n",
    "                    try:\n",
    "                        link = self.driver.find_element(By.CLASS_NAME, \"artdeco-button artdeco-button--primary artdeco-button--3 flex-shrink-zero\")\n",
    "                        link.click()\n",
    "                        time.sleep(4)  # Wait for the content to load after the click\n",
    "\n",
    "                        try:\n",
    "                            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                            \n",
    "                            ul_tag = soup.find('ul', class_='scaffold-layout__list-container')\n",
    "                            if ul_tag:\n",
    "                                li_tags = ul_tag.find_all('li')\n",
    "                                for li in li_tags:\n",
    "                                    div_tag = li.find('div', class_='flex-grow-1 artdeco-entity-lockup__content ember-view')\n",
    "                                    if div_tag:\n",
    "                                        job_title = div_tag.find('strong')\n",
    "                                        company = div_tag.find('span', class_='job-card-container__primary-description')\n",
    "                                        country_and_work_type = div_tag.find('li', class_='job-card-container__metadata-item')\n",
    "                                        \n",
    "                                        # Write data directly to CSV\n",
    "                                        writer.writerow([\n",
    "                                            (company.text).strip().replace('\\n', '') if company else \"N/A\",\n",
    "                                            (job_title.text).strip().replace('\\n', '') if job_title else \"N/A\", \n",
    "                                            (country_and_work_type.text).strip().replace('\\n', '') if country_and_work_type else \"N/A\"\n",
    "                                        ])\n",
    "                            else:\n",
    "                                print(\"UL tag not found for company URL: \", url)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing page source: {e}\")\n",
    "\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"The specific link was not found, skipping...\")\n",
    "                                                          \n",
    "        self.driver.quit()\n",
    "\n",
    "def read_csv(file_path):\n",
    "    with open(file_path, newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        entries = [row for row in reader]\n",
    "    return entries\n",
    "\n",
    "# Usage\n",
    "username = \"akber.anwer1234@gmail.com\"\n",
    "password = \"Makber@8472\"\n",
    "scraper = LinkedInScraper(username, password)\n",
    "scraper.login()\n",
    "\n",
    "entries = read_csv('path_to_modified_file_1_110.csv')\n",
    "scraper.fetch_jobs(entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n",
      "The specific link was not found, skipping...\n"
     ]
    }
   ],
   "source": [
    "class LinkedInScraper:\n",
    "    def __init__(self, email, password):\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.get('https://www.linkedin.com/login')\n",
    "        time.sleep(2)\n",
    "\n",
    "        email_elem = self.driver.find_element(By.ID, \"username\")\n",
    "        email_elem.send_keys(self.email)\n",
    "\n",
    "        password_elem = self.driver.find_element(By.ID, \"password\")\n",
    "        password_elem.send_keys(self.password)\n",
    "        password_elem.send_keys(Keys.RETURN)\n",
    "        time.sleep(2)\n",
    "        input(\"Please complete the captcha and login manually, then press Enter to continue...\")\n",
    "\n",
    "\n",
    "    def fetch_jobs(self, entries):\n",
    "        with open('Book1_251_jobs.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Company\", \"Job_Title\", \"Country_And_Work_Type\"])  # Header\n",
    "\n",
    "            for entry in entries:\n",
    "                url = entry['Domain']\n",
    "                if url:\n",
    "                    self.driver.get(url)\n",
    "                    time.sleep(5)  # Allow time for all scripts to load and data to display\n",
    "\n",
    "                    try:\n",
    "                        # Find the anchor tag and click it\n",
    "                        link = self.driver.find_element(By.CSS_SELECTOR, \"a.artdeco-button.artdeco-button--primary.artdeco-button--3.flex-shrink-zero\")\n",
    "                        link.click()\n",
    "                        time.sleep(4)  # Wait for the new tab to open\n",
    "\n",
    "                        # Handle the new window\n",
    "                        original_window = self.driver.current_window_handle\n",
    "                        new_window = [window for window in self.driver.window_handles if window != original_window][0]\n",
    "                        self.driver.switch_to.window(new_window)\n",
    "                        time.sleep(2)  # Allow some time for the new tab to load content\n",
    "\n",
    "                        # Scrape the data in the new window\n",
    "                        try:\n",
    "                            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                            ul_tag = soup.find('ul', class_='scaffold-layout__list-container')\n",
    "                            if ul_tag:\n",
    "                                li_tags = ul_tag.find_all('li')\n",
    "                                for li in li_tags:\n",
    "                                    div_tag = li.find('div', class_='flex-grow-1 artdeco-entity-lockup__content ember-view')\n",
    "                                    if div_tag:\n",
    "                                        job_title = div_tag.find('strong')\n",
    "                                        company = div_tag.find('span', class_='job-card-container__primary-description')\n",
    "                                        country_and_work_type = div_tag.find('li', class_='job-card-container__metadata-item')\n",
    "\n",
    "                                        writer.writerow([\n",
    "                                            (company.text).strip().replace('\\n', '') if company else \"N/A\",\n",
    "                                            (job_title.text).strip().replace('\\n', '') if job_title else \"N/A\", \n",
    "                                            (country_and_work_type.text).strip().replace('\\n', '') if country_and_work_type else \"N/A\"\n",
    "                                        ])\n",
    "                            else:\n",
    "                                print(\"UL tag not found for company URL: \", url)\n",
    "\n",
    "                        finally:\n",
    "                            self.driver.close()  # Close the new window\n",
    "                            self.driver.switch_to.window(original_window)  # Switch back to the original window\n",
    "\n",
    "                    except NoSuchElementException:\n",
    "                        print(\"The specific link was not found, skipping...\")\n",
    "                                                            \n",
    "        self.driver.quit()\n",
    "\n",
    "    \n",
    "def read_csv(file_path):\n",
    "    with open(file_path, newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        entries = [row for row in reader]\n",
    "    return entries\n",
    "\n",
    "# Usage\n",
    "username = \"akber.anwer1234@gmail.com\"\n",
    "password = \"Makber@8472\"\n",
    "scraper = LinkedInScraper(username, password)\n",
    "scraper.login()\n",
    "\n",
    "entries = read_csv('Book_1_251.csv')\n",
    "scraper.fetch_jobs(entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
